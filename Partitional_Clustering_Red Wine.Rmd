---
title: "Red_Wine"
output: html_document
date: "2024-09-25"
---
# Red Wine Quality Dataset

## Introduction

The red wine quality dataset can be found (https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009). This data set is related to red variants of the Portuguese "Vinho Verde" wine.It includes 11 physicochemical properties of 1599 observations and one target variable- Quality, stating a score for each observation.

## Dataset Description

All the 12 variable of the data set are numerical.

- **fixed acidity** : non-volatile acids in the wine, primarily tartaric acid 
- **volatile acidity**: Amount of acetic acid in the wine 
- **citric acid**: Amount of citric acid in the wine
- **residual sugar** :Sugar remaining after fermentation 
- **chlorides**:Salt content in the wine
- **free sulfur dioxide** : Amount of sulfur dioxide that is available to act as a preservative and antioxidant in the wine
- **total sulfur dioxide** :Overall sulfur dioxide content in the wine 
- **density** :Mass per unit volume of the wine 
- **pH** : Acidity of the wine 
- **sulphates** : Concentration of sulfates in the wine 
- **alcohol** : Ethanol content in the wine 
- **Quality** : Rating of the wine's overall quality (0-10) 

## Objective

The primary purpose of conducting clustering analyses using k-means and Partitioning Around Medoids (PAM) on the red wine dataset is to categorize wines into groups with similar physicochemical properties. 

```{r setup, message=FALSE, warning=FALSE}
library(tidyverse)
library(dplyr)
library(tidyr)
library(ggplot2)
library(corrplot)
library(skimr)
library(knitr)
library(stringr)
library(viridis)
library(factoextra)
library(hrbrthemes)
library(tibble)
library(forcats)
library(mclust)
library(fpc)
library(LPCM)
library(cluster)
library(NbClust)
library(funModeling)
library(clValid)
library(dbscan)
library(plotly)
```


```{r}
Rwine<-read.csv("C:/DOCUMENTS/LANGARA/3rd SEM/DANA 4840/Group Project/wine+quality/winequality-red.csv",sep = ";")
View(Rwine)
```

```{r}
str(Rwine)
head(Rwine)
colnames(Rwine)
```

##### Removing the Quality column, since it is used to verify the clusters.

```{r}
new_data <- Rwine[, -ncol(Rwine)]

# View the new data set
head(new_data)
```
```{r}
nrow(Rwine)
```


```{r}
library(hopkins)
set.seed(123)
```

### Hopkins Statistic 


```{r}
hopkins_value<-hopkins(new_data,m=(nrow(new_data)-1))
hopkins_value
```
##### Since Hopkins value is greater than 0.7 we can concluse that are data set is clusterable.

#Checking for missing values

```{r}
missing_values <- sapply(new_data, function(x) sum(is.na(x)))

print(missing_values)
```

There are no missing values in the data set.

</div>

# Exploratory Data Analysis

<div style="text-align: justify">


**Density Plot:**

```{r}
options(scipen = 999)

density_wine <- ggplot(Rwine, aes(x = quality)) + geom_density()

density_wine + 
  geom_vline(xintercept = 5, col = "red", size = 2) + 
  geom_vline(xintercept = 6, col = "blue", size = 2)
```

<p>A simple density plot on the dataset shows that our dataset has 2 bumps (considering threshold density as 0.5 for significance), suggesting that there are more number of data for quality 5 and 6.</p>


**Correlation Plot:**

```{r}
corrplot(cor(Rwine))
```

Alcohol vs. Quality: Shows a strong positive correlation, indicating that higher alcohol content tends to be associated with better wine quality.

Density vs. Alcohol: Displays a negative correlation, suggesting wines with higher alcohol tend to have lower density.

Citric Acid vs. Fixed Acidity: Strong positive correlation

pH vs. Fixed Acidity: Negative correlation, 

Free Sulfur Dioxide vs. Total Sulfur Dioxide: High positive correlation, as expected since these variables are closely related.


**Histogram Analysis:**

```{r}
plot_num(Rwine)
```

From the histograms, we observe that:

1. The pH value seems to dispaly a normal distribution with major samples exhibiting values between 3.0 and 3.5.

2. The free sulfur dioxide seems to be between the 1-72 count with peaking around 10 mark.

3. The total sulfur dioxide seems to a have a spread between 0 and 175 and exhibiting peak around 50.

4. The alcohol content seems to vary from 8 to 14 with major peaks around 10 with a lower count between 13 and 14.

5. The fixed acidity, volatile acidity and density can almost be considered to be normally distributed.

6. Majorly, the variables distributions are right-tailed.

**Boxplot Analysis:**

```{r}
box_colors <- c("lightblue", "lightgreen", "lightcoral", "lightpink", "lightyellow", "lightgray")


boxplot(scale(Rwine),
        xlab = "Value", 
        ylab = "Parameters", 
        main = "Boxplot Presentation of Different Parameters",
        col = box_colors,         
        border = "black",           
        notch = TRUE,             
        outline = TRUE,            
        )          

```

<p>We have scaled all the values for boxplot analysis to bring them at similar scales and avoid overrepresntation of any independant parameter. A simple analysis of the boxplot shows that there are major outliers in residual sugar, chlorides and sulphates and minor outliers in citric acid.</p>


## Mean value for each Chemical character

```{r}

# Group by quality and calculate the mean for each variable
grouped_by_quality <- Rwine %>%
  group_by(quality) %>%
  summarise(across(everything(), mean))

print(grouped_by_quality)

```

The mean values are almost similar for quality level 7,8 for all the variable except alcohol. 

A clear difference is shown in citric acid levels in lower quality wines and higher quality wines.

A clear difference is also seen in the alochol level of lower quality wines and higher quality wines. However the quality 4 has a higher alcohol level than quality 5. Since the values are almost similar with clear distinctivity, this might lead to not showing clear clusters when K means algorthim is applied. 



#Scaling the data

```{r}
df.scaled<-scale(new_data)
```

# Visualizing Distance Matrix 

```{r}
library(stats)
library(factoextra)

dist.eucl<-dist(df.scaled,method="euclidean")
fviz_dist(dist.eucl)
```

#PCA - 

Visualizing the data to assess whether the contain any meaningful clusters.We perform PCA to reduce the dimensionality to plot. 
```{r}
library("factoextra")

fviz_pca_ind(prcomp(df.scaled),title="PCA-Wine",habillage = Rwine$quality,palette = "jco",geom="point",ggtheme=theme_classic(),legend="bottom")
```

This plot visualizes the results of a Principal Component Analysis (PCA) applied to a dataset.The colors and shapes represent different levels of wine quality.
In this plot, it seems difficult to identify clear clusters, suggesting the wine samples may not separate distinctly based on these components.


# Determing the optimal number of clusters

# 1) Elbow method

```{r}
wss <- numeric(10)
for (i in 1:10) {
  wss[i] <- sum(kmeans(df.scaled, centers = i)$tot.withinss)
}

plot(1:10, wss, type = "b", pch = 16, col = "blue", lwd = 2, 
     xlab = "Number of clusters k", ylab = "Total Within Sum of Squares")

grid()

for (i in 1:10) {
  text(i, wss[i], labels = round(wss[i], 1), pos = 3, cex = 0.8, col = "black")
  points(i, wss[i], col = rgb(0, 0, 1, alpha = i/10), pch = 16, cex = 2)
}
```

The Elbow appears to occur around 3 or 4 clusters, as the rate of decrease in WSS slows down after that point. 

# 2) Silhoutte method

```{r}
fviz_nbclust(df.scaled, kmeans, method = "silhouette") +
  labs(subtitle = "Silhouette method") +
  theme_minimal() + 
    geom_line(size = 3, color = "blue") +  
  geom_point(size = 3, color = "blue")     

```

The silhouette method suggest 2 clusters as optimal.

# 3) Gap Statistic

```{r}
set.seed(123)
fviz_nbclust(df.scaled, kmeans, nstart = 25, method = "gap_stat", nboot = 50) +
  labs(subtitle = "Gap Statistic method") +
  theme_minimal() +                # Optional: Cleaner theme
  geom_line(size = 1.5, color = "blue")  # Thicker lines, color set to blue

```

The Gap Statistic method suggest 3 clusters as optimal. 

# 4) Nbclust function

```{r}
library("NbClust")
library("factoextra")

nb<-NbClust(df.scaled,distance="euclidean",min.nc=2,max.nc=6,method="kmeans")
```
The NbClust function proposes 2 as the best number of clusters. 

```{r}
table(Rwine$quality)
```
There are 6 quality values in the target variable, suggesting there should be 6 clusters.

Hence we performed K means algorithm for cluster number 2,3 and 6 as suggested by the above plots and analysis, to identify the ideal number of clusters. 

#### Explaination for the number of clusters


# K Means clustering

### 3 Clusters
```{r}
set.seed(123)
library("factoextra")

km.res3<-eclust(df.scaled,"kmeans",k=3,nstart=25,graph = FALSE)

km.res3
```
```{r}
library("factoextra")

fviz_cluster(km.res3,geom="point",ellipse.type = "norm",
             palette="jco",ggtheme = theme_minimal())
```

There is some overlap between the clusters, especially between Cluster 2 (yellow) and Cluster 3 (gray). This suggests that these clusters share similar characteristics in the reduced dimensional space.
Cluster 1 (blue) appears more distinct, indicating better separation from the other clusters.
Overlapping points between clusters imply that these clusters are not perfectly distinct in this reduced dimensional space. There may be similarities in the original dataset among these clusters.

### 2 Clusters
```{r}
library("factoextra")

km.res2<-eclust(df.scaled,"kmeans",k=2,nstart=25,graph = FALSE)

km.res2
```
```{r}
library("factoextra")

fviz_cluster(km.res2,geom="point",ellipse.type = "norm",
             palette="jco",ggtheme = theme_minimal())
```

The ellipses for the two clusters overlap significantly, indicating that there is no clear separation between the two clusters in this 2D representation.This may be because, the clusters are not well-separated in the reduced dimensional space, the data have overlapping features.

### 6 clusters

```{r}
library("factoextra")

km.res6<-eclust(df.scaled,"kmeans",k=6,nstart=25,graph = FALSE)

km.res6
```


```{r}
library("factoextra")

fviz_cluster(km.res6,geom="point",ellipse.type = "norm",
             palette="jco",ggtheme = theme_minimal())
```

# Cluster validation 

### 1) 2 Clusters

#### Silhoutte Coefficient 

```{r}
library(factoextra)
library(cluster)

# Visualize the silhouette plot
fviz_silhouette(km.res2, palette = "jco", ggtheme = theme_classic())
```

The silhouette plot above shows negative values suggesting there are points that are misclassified in cluster 1, as they are closer to cluster 2. 

```{r}
silinfo2<-km.res2$silinfo

#Average silhouette width of each cluster (2 clusters)
silinfo2$clus.avg.widths
```
```{r}
#Finding the points with negative width

sil2<-km.res2$silinfo$widths[,1:3]

neg.sil<-which(sil2[,'sil_width']<0)
sil2[neg.sil, ,drop=FALSE]
```

The table above give a summary of the negative silhouette points. There are 109 data points that misclassified. 

#### Dunn Index

```{r}
km_stats2<-cluster.stats(dist(df.scaled),km.res2$cluster)

km_stats2$dunn
```
#### Connectivity Score

```{r}
connectivity_score <- connectivity(distance = dist(df.scaled), clusters = km.res2$cluster)

# Print the connectivity score
print(connectivity_score)
```

```{r}
library(factoextra)
library(cluster)

# Visualize the silhouette plot
fviz_silhouette(km.res3, palette = "jco", ggtheme = theme_classic())
```

A higher silhouette width indicates better-defined clusters. Although the difference is not large, the silhouette width suggests that 2 clusters might be slightly better in terms of cluster quality.

```{r}
silinfo3<-km.res3$silinfo

#Average silhouette width of each cluster (3 clusters)
silinfo3$clus.avg.widths
```
#### Dunn Index

```{r}
km_stats3<-cluster.stats(dist(df.scaled),km.res3$cluster)

km_stats3$dunn
```

```{r}
library(factoextra)
library(cluster)

fviz_silhouette(km.res6, palette = "jco", ggtheme = theme_classic())
```

```{r}
km_stats6<-cluster.stats(dist(df.scaled),km.res6$cluster)

km_stats6$dunn
```

```{r}
connectivity_score6 <- connectivity(distance = dist(df.scaled), clusters = km.res6$cluster)

# Print the connectivity score
print(connectivity_score6)
```
# Internal Validation Summary 

```{r}
library(clValid)

clmethodsK<-c("kmeans")

internK<-clValid(df.scaled,nClust = 2:6,clMethods = clmethodsK,validation = "internal",metric = "euclidean",maxitems = 1599)

summary(internK)
```
```{r}
op<-par(no.readonly=TRUE)
par(mfrow=c(2,2),mar=c(4,4,3,1))
plot(internK, legend=FALSE)
plot(nClusters(internK), measures(internK, "Dunn")[,,1], type="n", axes=F, xlab="", ylab="")
legend("center", clusterMethods(internK), col=1:9, lty=1:9, pch=paste(1:9))
```

The Internal Validation summary says that connectivity is lowest for 3 clusters, Dunn Index is highest for 6 clusters. 

# Stability

```{r}
library(clValid)
stab<-clValid(df.scaled,nClust = 2:6,clMethods = clmethodsK,validation = "stability",maxitems = 1599)
summary(stab)
```

```{r}
par(mfrow=c(2,2),mar=c(4,4,3,1))
plot(stab, measures=c("APN","AD","ADM","FOM"),legend=FALSE)
plot(nClusters(stab), measures(stab, "APN")[,,1], type="n", axes=F, xlab="", ylab="")
legend("center", clusterMethods(stab), col=1:9, lty=1:9, pch=paste(1:9))
```

The Stability scores above says APN and ADM suggest 2 clusters as best and AD and FOM suggest 6 clusters. 


# External Validation

## 1) Ajusted Rand Index (ARI)

```{r}
true_labels <- Rwine$quality
```

```{r}
library("fpc")

quality <- as.numeric(Rwine$quality)
clust_stats3 <- cluster.stats(d = dist(df.scaled), 
                             quality, km.res3$cluster)
clust_stats3$corrected.rand
```
```{r}
library("fpc")

clust_stats2 <- cluster.stats(d = dist(df.scaled), 
                             quality, km.res2$cluster)
clust_stats2$corrected.rand
```

```{r}
library("fpc")

clust_stats6 <- cluster.stats(d = dist(df.scaled), 
                             quality, km.res6$cluster)
clust_stats6$corrected.rand
```
#### ARI Plot

```{r}
ari_values <- c(clust_stats2$corrected.rand, clust_stats3$corrected.rand, clust_stats6$corrected.rand)

ari_results <- data.frame(
  Clusters = c(2, 3, 6),
  ARI = ari_values
)

library(ggplot2)

ggplot(ari_results, aes(x = factor(Clusters), y = ARI, group = 1)) +
  geom_line(color = "blue", size = 1) +      
  geom_point(color = "red", size = 3) +      
  geom_text(aes(label = round(ARI, 3)),      
            vjust = -0.5, size = 4, color = "black") + 
  theme_minimal() +
  labs(title = "Adjusted Rand Index (ARI) for Different Cluster Counts",
       x = "Number of Clusters",
       y = "Adjusted Rand Index (ARI)") +
  theme(axis.text.x = element_text(size = 12), 
        axis.text.y = element_text(size = 12),
        plot.title = element_text(hjust = 0.5))
```

The plot above for ARI suggest 6 clusters as it similar to number of quality values in our ground truth. 

## 2) Meila's Index (Using the formula)

```{r}
meilas_index <- function(true_labels, cluster_labels) {
  N <- length(true_labels)
  M <- 0
  
  # Loop over each pair of points (i, j)
  for (i in 1:(N-1)) {
    for (j in (i+1):N) {
      # Indicator functions for Meila's index
      I_c1_diff <- ifelse(true_labels[i] != true_labels[j], 1, 0)  # c1(i) != c1(j)
      I_c2_eq <- ifelse(cluster_labels[i] == cluster_labels[j], 1, 0)  # c2(i) == c2(j)
      
      # Update M based on indicator functions
      M <- M + I_c1_diff * I_c2_eq
    }
  }
  
  # Normalize by N * (N - 1)
  mi_score <- M / (N * (N - 1))
  
  return(mi_score)
}
```


### 2 clusters

```{r}
cluster_labels2<- km.res2$cluster
meila_index2 <- meilas_index(true_labels, cluster_labels2)

print(meila_index2)
```

### 3 clusters

```{r}
cluster_labels3<- km.res3$cluster
meila_index3 <- meilas_index(true_labels, cluster_labels3)

print(meila_index3)
```
### 6 clusters
```{r}
cluster_labels6<- km.res6$cluster
meila_index6 <- meilas_index(true_labels, cluster_labels6)

print(meila_index6)
```

## Meil's Index Plot

```{r}
mi_results <- data.frame(
  Clusters = c(2, 3, 6),
  MeilaIndex = c(meila_index2, meila_index3, meila_index6)
)

# Plotting the Meila's Index for different cluster counts as a line plot
library(ggplot2)

ggplot(mi_results, aes(x = factor(Clusters), y = MeilaIndex, group = 1)) +
  geom_line(color = "skyblue", size = 1) +  
  geom_point(color = "blue", size = 3) +    
  geom_text(aes(label = round(MeilaIndex, 3)),  
            vjust = -0.5, size = 4, color = "darkred") + 
  theme_minimal() +
  labs(title = "Meila's Index for Different Cluster Counts",
       x = "Number of Clusters",
       y = "Meila's Index") +
  theme(axis.text.x = element_text(size = 12), 
        axis.text.y = element_text(size = 12),
        plot.title = element_text(hjust = 0.5))
```
The plot above for Meila's Index suggest 6 clusters as it similar to number of quality values in our ground truth.


# K medoids

#### Estimating the optimal number of clusters using PAM

1. Elbow method
```{r}
library(factoextra)
library(cluster)

fviz_nbclust(df.scaled, pam, method = "wss") +
  labs(subtitle = "Elbow Method for Optimal Clusters (PAM)")
```
**Interpretation:**

From the plot we see that the elbow is created at 4 so the optimal number of clusters is 4.


2. Silhouette method
```{r}
library(cluster)
library(factoextra)
fviz_nbclust(df.scaled, pam, method = "silhouette")+
  theme_classic()
```
**Interpretation:**

For the silhouette method the optimal number of clusters suggested is 2.

3. Gap Statistic
```{r}
set.seed(123)
fviz_nbclust(df.scaled, pam, method = "gap_stat", nboot = 50) +
  labs(subtitle = "Gap Statistic Method with PAM")
```
**Interpretation:**

From gap statistic we get k=10 optimal clusters.

## Conclusion
From all the different methods that we saw to apply PAM on our dataset, we can conclude that we do not get accurate results for optimal number of clusters.

Each method gives a different answer. We still tried the optimal k value suggested by Silhouette method (k=2) and Elbow method (k=4). The Gap statistic method suggests k=10 optimal clusters which seems unnecessary and hence we did not apply that.

Instead we applied k=6 clusters (based on ground truth variable) to compare how it will be different from the optimal clusters suggested by above methods.

## Applying PAM for k=4
```{r}
library(factoextra)
pam.res1 = eclust(df.scaled, "pam", k=4, graph=FALSE)
pam.res1
```

```{r}
fviz_cluster(list(data=df.scaled, cluster=pam.res1$clustering),
             ellipse.type = "norm", geom = "point", stand = FALSE, 
             palette= "jco", ggtheme = theme_classic())
```

## PAM at k=2
```{r}
pam.res2 = eclust(df.scaled, "pam", k=2, graph=FALSE)
pam.res2
```

```{r}
fviz_cluster(list(data=df.scaled, cluster=pam.res2$clustering),
             ellipse.type = "norm", geom = "point", stand = FALSE, 
             palette= "jco", ggtheme = theme_classic())
```

## PAM at k=6
```{r}
pam.res3 = eclust(df.scaled, "pam", k=6, graph=FALSE)
pam.res3
```

```{r}
fviz_cluster(list(data=df.scaled, cluster=pam.res3$clustering),
             ellipse.type = "norm", geom = "point", stand = FALSE, 
             palette= "jco", ggtheme = theme_classic())
```

**Interpretation:**

After applying pam on k=4, 2 and 6 we see that neither of the one is able to form separate clusters. The points are overlapping from one cluster to another.

The reason for this could be due to the ground truth variable (Quality). We see that quality variable has 6 groups and so we have created clusters with 6 groups as well. But wine quality can be determined by various factors and quality variable can be a factor not a ground truth due to which we are not able to form seperate clusters.

We will still proceed with the further classifications to see how it will interpret our dataset.


## Internal Classification

Silhouette plot
```{r}
fviz_silhouette(pam.res1, palette="jco", ggtheme= theme_classic())
```


```{r}
fviz_silhouette(pam.res2, palette="jco", ggtheme= theme_classic())
```

```{r}
fviz_silhouette(pam.res3, palette="jco", ggtheme= theme_classic())
```

**interpretation:** 
Average Silhouette width for each cluster is seen in the silhouette plot.
k=2 clusters has highest silhouette width of 0.21, where as plot with k=4 clusters has average width of 0.17 and plot with k=6 clusters has lowest average so=ilhouette width of 0.13.

So this suggests we choose k=2 clusters which is having maximum average silhouette width.


## Dunn Index using PAM
```{r}
library(fpc)
stats1 = cluster.stats(dist(df.scaled), pam.res1$cluster)
stats1$dunn

stats2 = cluster.stats(dist(df.scaled), pam.res2$cluster)
stats2$dunn

stats3 = cluster.stats(dist(df.scaled), pam.res3$cluster)
stats3$dunn
```
**Interpretation:**

The Dunn Index value for each cluster result is below.

for k=4, Dunn Index = 0.02178405
for k=2, Dunn Index = 0.02435284
for k=6, Dunn Index = 0.01936127


##Computing ClValid

#### Internal Measures
```{r}
library(clValid)
clmethods <- c("pam")
intern_pam = clValid(df.scaled, nClust =2:6, clMethods= clmethods, validation="internal",maxitems = 1599)
summary(intern_pam)
```
**Interpretation:** 

As per the internal validation measure it suggests that for connectivity and Silhouette 2 clusters perform better where as for Dunn Index 3 clusters perform better.

#### Internal Measure plots
```{r}
op = par(no.readonly=TRUE)
par(mfrow=c(2,2),mar=c(4,4,3,1))
plot(intern_pam, legend=FALSE)
plot(nClusters(intern_pam),measures(intern_pam,"Dunn")[,,1],type="n",axes=F,xlab="",ylab="")
legend("center", clusterMethods(intern_pam),col=1:9, lty=1:9,pch=paste(1:9))
par(op)
```


#### Stability measures
```{r}
stab_pam <- clValid(df.scaled, nClust = c(2,4,6), clMethods = clmethods, validation = "stability",maxitems = 1599)
optimalScores(stab_pam)
```
**Interpretation:**

For APN and ADM measures, PAM with 3 clusters gives the best score and for other measures PAM with 6 clusters has the best score.

#### Stability Measure plots
```{r}
par(mfrow=c(2,2),mar=c(4,4,3,1))
plot(stab_pam,measure = c("APN","AD","ADM","FOM"), legend= FALSE)
plot(nClusters(stab_pam),measures(stab_pam,"APN")[,,1],type="n",axes=F,xlab="",ylab="")
legend("center", clusterMethods(stab_pam),col=1:9, lty=1:9,pch=paste(1:9))
par(op)
```


## External Validation
```{r}
table(Rwine$quality, pam.res1$cluster)
```
From the ground truth variable table for k=4 clusters we see that the maximum groupings is for quality 5 and 6.

## Corrected Rand Index
```{r}
library(fpc)
quality <- as.numeric(Rwine$quality)
cs1= cluster.stats(d = dist(df.scaled), quality, pam.res1$cluster)
ari1= cs1$corrected.rand
ari1

cs2= cluster.stats(d = dist(df.scaled), quality, pam.res2$cluster)
ari2= cs2$corrected.rand
ari2

cs3= cluster.stats(d = dist(df.scaled), quality, pam.res3$cluster)
ari3= cs3$corrected.rand
ari3
```
**Interpretation:**  The corrected Rand index value shows that agreement between quality type and cluster solution is 0.06454827. It has a range of -0.5 to 1 and value of 0.0645 shows a very low index value.

```{r}
k_values <- c(2, 4, 6)  
ari_values <- c(ari1, ari2, ari3)

plot(k_values, ari_values,
     main = "Adjusted Rand Index for different k",
     xlab = "Number of Clusters (k)",
     ylab = "Adjusted Rand Index",
     xlim = c(1, 7), ylim = c(0, 1),
     pch = 19, col = "blue")
lines(k_values, ari_values, col = "red", lwd = 1)
text(k_values, ari_values, 
     labels = sprintf("ARI=%.4f", ari_values), 
     pos = 3, col = "black")
```

**Interpretation:** We have 2 ARI values that are similar and highest value suggests that k=6 as ground truth variable has 6 types.


## Meila's VI
```{r}
meilas_index <- function(true_labels, cluster_labels) {
  N <- length(true_labels)
  M <- 0
  for (i in 1:(N-1)) {
    for (j in (i+1):N) {
      I_c1_diff <- ifelse(true_labels[i] != true_labels[j], 1, 0)  
      I_c2_eq <- ifelse(cluster_labels[i] == cluster_labels[j], 1, 0)
      M <- M + I_c1_diff * I_c2_eq
    }
  }
  mi_score <- M / (N * (N - 1))
  return(mi_score)
}
```


```{r}
cluster_label1 <- pam.res1$cluster
true_label <- Rwine$quality
meila_index1 <- meilas_index(true_label, cluster_label1)
print(meila_index1)

cluster_label2 <- pam.res2$cluster
meila_index2 <- meilas_index(true_label, cluster_label2)
print(meila_index2)

cluster_label3 <- pam.res3$cluster
meila_index3 <- meilas_index(true_label, cluster_label3)
print(meila_index3)
```
**Interpretation:**
The agreement between the quality types and cluster solution is 0.0837245 using Meila's VI.

```{r}
k_values <- c(2, 4, 6)  
vi_values <- c(meila_index1, meila_index2, meila_index3)

plot(k_values, vi_values,
     main = "Meila's Index for different k",
     xlab = "Number of Clusters (k)",
     ylab = "Meila's Index",
     xlim = c(1, 7), ylim = c(0, 1),
     pch = 19, col = "blue")
lines(k_values, vi_values, col = "red", lwd = 1)
text(k_values, vi_values, 
     labels = sprintf("ARI=%.4f", vi_values), 
     pos = 3, col = "black")
```

**Interpreattion:** 

The lowest Meila's VI value 0.0601, suggests that k=6. This can be due to ground truth variable which has 6 categories.

